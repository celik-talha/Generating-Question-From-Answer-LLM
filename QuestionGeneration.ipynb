{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckWKh_LnryLw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install torch transformers accelerate tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Data loading and tokenizer setup\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, AdamW, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data_path1 = '/content/drive/MyDrive/AraProje/QuestionPrediction/training_data.csv'\n",
        "data_path2 = '/content/drive/MyDrive/AraProje/QuestionPrediction/validation_data.csv'\n",
        "data1 = pd.read_csv(data_path1)\n",
        "data2 = pd.read_csv(data_path2)\n",
        "data1 = data1.iloc[:2000]\n",
        "data2 = data2.iloc[:500]\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('ytu-ce-cosmos/turkish-gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Block 2: Data preparation function\n",
        "def prepare_sequence(input_text, question_text):\n",
        "    # For input: reverse the tokens and convert back to text\n",
        "    input_tokens = tokenizer.encode(input_text, add_special_tokens=True)\n",
        "    reversed_input_tokens = input_tokens[::-1]\n",
        "    reversed_input_text = tokenizer.decode(reversed_input_tokens)\n",
        "\n",
        "    # Tokenize both for model input\n",
        "    input_encoding = tokenizer(reversed_input_text,\n",
        "                             max_length=128,\n",
        "                             padding='max_length',\n",
        "                             truncation=True,\n",
        "                             return_tensors='pt')\n",
        "\n",
        "    return input_encoding, input_encoding\n",
        "\n",
        "# Block 3: Prepare datasets\n",
        "train_data = data1\n",
        "val_data = data2\n",
        "\n",
        "# Process training data\n",
        "train_inputs = []\n",
        "train_labels = []\n",
        "train_masks = []\n",
        "\n",
        "for _, row in tqdm(train_data.iterrows(), desc=\"Processing training data\"):\n",
        "    input_encoding, label_encoding = prepare_sequence(row['input'], row['label'])\n",
        "\n",
        "    train_inputs.append(input_encoding['input_ids'].squeeze())\n",
        "    train_labels.append(label_encoding['input_ids'].squeeze())\n",
        "    train_masks.append(input_encoding['attention_mask'].squeeze())\n",
        "\n",
        "# Convert to tensors\n",
        "train_inputs = torch.stack(train_inputs)\n",
        "train_labels = torch.stack(train_labels)\n",
        "train_masks = torch.stack(train_masks)\n",
        "\n",
        "# Process validation data similarly\n",
        "val_inputs = []\n",
        "val_labels = []\n",
        "val_masks = []\n",
        "\n",
        "for _, row in tqdm(val_data.iterrows(), desc=\"Processing validation data\"):\n",
        "    input_encoding, label_encoding = prepare_sequence(row['input'], row['label'])\n",
        "\n",
        "    val_inputs.append(input_encoding['input_ids'].squeeze())\n",
        "    val_labels.append(label_encoding['input_ids'].squeeze())\n",
        "    val_masks.append(input_encoding['attention_mask'].squeeze())\n",
        "\n",
        "val_inputs = torch.stack(val_inputs)\n",
        "val_labels = torch.stack(val_labels)\n",
        "val_masks = torch.stack(val_masks)\n",
        "\n",
        "# Block 4: Create dataloaders\n",
        "train_dataset = TensorDataset(train_inputs, train_labels, train_masks)\n",
        "val_dataset = TensorDataset(val_inputs, val_labels, val_masks)\n",
        "\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "r1uJI8eksCSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/AraProje/PreviousTokenPrediction/TersGpt2Large'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path, from_tf = True)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.to('cuda')"
      ],
      "metadata": {
        "id": "PweOiG9rsIeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizasyon ve scheduler tanımlama\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "num_training_steps = len(train_dataloader) * 2  #epoch\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n",
        "\n",
        "# Loss fonksiyonu tanımlama\n",
        "from torch.nn import CrossEntropyLoss\n",
        "loss_fn = CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "fOG4Aswq6ruf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 3\n",
        "best_val_loss = float('inf')\n",
        "patience = 4\n",
        "early_stop_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ---- Eğitim döngüsü ----\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_loop = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    for batch in train_loop:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch[0].to('cuda')\n",
        "        labels = batch[1].to('cuda')\n",
        "        attention_mask = batch[2].to('cuda')\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # ---- Validation döngüsü ----\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch[0].to('cuda')\n",
        "            labels = batch[1].to('cuda')\n",
        "            attention_mask = batch[2].to('cuda')\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping ve model checkpointing\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        early_stop_counter = 0\n",
        "        model.save_pretrained('/content/drive/MyDrive/AraProje/QuestionPrediction/Test')\n",
        "        tokenizer.save_pretrained('/content/drive/MyDrive/AraProje/QuestionPrediction/Test')\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "print(\"Eğitim tamamlandı ve model kaydedildi.\")"
      ],
      "metadata": {
        "id": "mBjSdQkOsKdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1601e28d-061d-4569-acc8-e4b65cc9c093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:   0%|          | 0/125 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "Epoch 1/3: 100%|██████████| 125/125 [01:25<00:00,  1.46it/s, loss=1.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Train Loss: 1.5323, Validation Loss: 1.4725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 125/125 [01:24<00:00,  1.47it/s, loss=1.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Train Loss: 1.2519, Validation Loss: 1.4942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 125/125 [01:24<00:00,  1.47it/s, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.8907, Validation Loss: 1.5950\n",
            "Eğitim tamamlandı ve model kaydedildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Tokenizer'ı yükleme\n",
        "tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/AraProje/QuestionPrediction/Model2')\n",
        "\n",
        "# Modeli yükleme\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/AraProje/QuestionPrediction/Model2')\n",
        "model.to('cuda')  # Eğer GPU kullanıyorsanız modeli CUDA'ya taşıyın"
      ],
      "metadata": {
        "id": "kRcRLHvMzV5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "text = \"\"\"Türkiyenin başkenti Ankara'dır\"\"\"\n",
        "text = \" Cevap: \" + text\n",
        "d = tokenizer.decode(tokenizer.encode(text)[::-1], skip_special_tokens = True)\n",
        "\n",
        "\n",
        "text_generator = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    #top_k=40,               # En olası 50 token arasından seçim yap\n",
        "    #top_p=0.90,              # Kümülatif olasılığı %90 olan tokenlar arasından seçim yap\n",
        "    #temperature=0.5,        # Daha tutarlı sonuçlar için düşük sıcaklık değeri\n",
        "    #repetition_penalty=2.0  # Tekrar eden kelimelere ceza uygula\n",
        ")\n",
        "r = text_generator(d, max_length=100,truncation=True)[0]['generated_text']\n",
        "tokenizer.decode(tokenizer.encode(r)[::-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XknkBikBDF6Z",
        "outputId": "c9fb7536-f49a-4364-be7c-eb5ae97c66dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Soru: Hangi ülkenin başkent hangileridir? Cevap: Türkiyenin başkenti Ankara'dır\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}